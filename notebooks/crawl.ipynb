{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web scraping and crawling\n",
    "Uses the *BeautifulSoup* and *Selenium* external packages.\n",
    "[BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup / Data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "from util import utility\n",
    "from settings import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting started"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Website to work with, i.e. to scrape info from and crawl over it - Ultimate Classic Rock.\n",
    "The starting URL refers to articles about Paul McCartney."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_url = 'https://ultimateclassicrock.com/search/?s=paul%20mccartney'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create `Response` object from GET request, using `requests.get(<url>, allow_redirects=False)`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = requests.get(start_url)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get response text from `Response` object, using `<response>.text`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response_text = response.text\n",
    "print(response_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get BeautifulSoup object from response text, using `BeautifulSoup(<response text>, 'html.parser')`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response_text, 'html.parser')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `get_soup(url)` function for getting a BeautifulSoup object."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_soup(url: str) -> BeautifulSoup:\n",
    "    \"\"\"Returns BeautifulSoup object from the corresponding URL, passed as a string.\n",
    "    Creates Response object from HTTP GET request, using requests.get(<url string>, allow_redirects=False),\n",
    "    and then uses the text field of the Response object and the 'html.parser' to create the BeautifulSoup object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create Response object from HTTP GET request; assume that no redirection is allowed (allow_redirects=False)\n",
    "    response = requests.get(url, allow_redirects=False)\n",
    "    # Get text from the Response object, using <response>.text\n",
    "    response_text = response.text\n",
    "    # Create and return the corresponding BeautifulSoup object from the response text; use features='html.parser'\n",
    "    return BeautifulSoup(response_text, 'html.parser')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_soup(url)`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup = get_soup(start_url)\n",
    "print(soup)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save BeautifulSoup object to an HTML file, using `<Path-file-object>.write_text(str(<BeautifulSoup object>), encoding='utf-8', errors='replace')`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup_file = DATA_DIR / 'soup.html'\n",
    "soup_file.write_text(str(soup), encoding='utf-8', errors='replace')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate `<BeautifulSoup object>.find('<tag>')`; e.g., find the first `<span>` tag."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(soup.find('span'))\n",
    "print(soup.span)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate `<BeautifulSoup object>.find('<tag>').find('<nested tag>')`; e.g., find the `<a>` tag in an `<article>` tag."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(soup.article, '\\n')\n",
    "print(soup.article.a, '\\n')\n",
    "print(soup.article.a.figcaption, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate getting a tag with specific attributes using `<BeautifulSoup object>.find('<tag>', {'<attribute>': '<value>'})`; e.g.:\n",
    "- find a `<span>` tag with the `visually-hidden` attribute\n",
    "- find an `<article>` tag with the `title` attribute"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(soup.find('span', {'class': 'visually-hidden'}))\n",
    "print(soup.find('article').find('a', {'class': 'title'}))\n",
    "print(soup.find('article').find('a', {'class': 'theframe'}))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate getting values of tag attributes, e.g. `<BeautifulSoup object>.find('<tag>').text` for an `<a>` tag and for a `<visually-hidden>` tag.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(soup.article, '\\n')\n",
    "print(soup.article.a, '\\n')\n",
    "print(soup.article.a.text, '\\n')\n",
    "print(soup.find('span', {'class': 'visually-hidden'}), '\\n')\n",
    "print(soup.find('span', {'class': 'visually-hidden'}).text, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate `<BeautifulSoup object>.find_all(<tag>)`, e.g. for the `<article>` tag; returns a `ResultSet` object. Show the entire object, its type, and loop through the ses to show individual articles."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles = soup.find_all('article')\n",
    "for article in articles:\n",
    "    print(article, '\\n')\n",
    "print(type(articles))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following lines demonstrate that getting the soup with `requests.get()` does not capture all tags (those filled with JavaScript, e.g. `<time>`). In this example, these tags can be found in `div` tags (`{'class': 'auth-date'}`). Try to find it using `find()` or using `findNext()`.\n",
    "\n",
    "That's when using `selenium.webdriver` is better."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(soup.find('div', {'class': 'auth-date'}))\n",
    "print(soup.find('div', {'class': 'auth-date'}).time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Selenium version, needed for extracting the `<time>` tag info."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Before running the following line, make sure to download and unzip THE LATEST VERSION of chromedriver\n",
    "# (or the version COMPATIBLE with your version of Chrome)\n",
    "# and put chromedriver.exe in the Scripts subfolder of your Python installation folder,\n",
    "# e.g. C:\\Users\\Vladan\\AppData\\Local\\Programs\\Python\\Python310\\Scripts.\n",
    "# The driver should be downloaded from https://chromedriver.chromium.org/downloads.\n",
    "# Then you need not provide the path of the driver, just run: driver = webdriver.Chrome().\n",
    "# (Adapted from https://stackoverflow.com/a/60062969/1899061.)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(start_url)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(start_url)\n",
    "soup = BeautifulSoup(str(driver.page_source), 'html.parser')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the BeautifulSoup object to an HTML file, using `<Path-file-object>.write_text(str(<BeautifulSoup object>), encoding='utf-8', errors='replace')`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup_file.write_text(str(soup), errors='replace', encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `get_soup_selenium(url)` function for getting a BeautifulSoup object using the `selenium` package instead of `requests`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_soup_selenium(url: str) -> BeautifulSoup:\n",
    "    \"\"\"Returns BeautifulSoup object from the corresponding URL, passed as a string.\n",
    "    Makes an HTTP GET request, using driver = webdriver.Chrome() from the selenium package and its driver.get(url).\n",
    "    Then uses the page_source field of the driver object and the 'html.parser' to create and return the BeautifulSoup o.\n",
    "    \"\"\"\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    return BeautifulSoup(driver.page_source, 'html.parser')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_soup_selenium(url)`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup = get_soup_selenium(start_url)\n",
    "print(soup)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(soup.find('div', {'class': 'auth-date'}))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate occasional anomalies in the ResultSet returned by `<BeautifulSoup object>.find_all(<tag>)`; note that they may be appearing only in the `selenium` version, not in the `requests` version."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The following lines show that there are 11 articles on the page, not 10.\n",
    "# The 11th one is something else, not visible on the page at the first glance and should be eliminated from\n",
    "# further processing.\n",
    "\n",
    "# print(len(soup.find_all('article')))\n",
    "articles = soup.find_all('article')\n",
    "for article in articles:\n",
    "    print(article, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following line shows an anomaly in the articles `ResultSet`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(articles[len(articles) - 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare it to any of the other results from the `ResultSet` returned by `<BeautifulSoup object>.find_all(<tag>)`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(articles[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate different ways of getting an attribute value for a tag (a `bs4.element.Tag` object):\n",
    "- `<tag>.find('<subtag>')`, filtered with `<{'class': \"<class name>\"}>`\n",
    "- `<tag>.find('<subtag>')['<attr>']`\n",
    "- `<tag>.find('<subtag>').get('<attribute>')`\n",
    "- `<tag>.find('<subtag>').attrs['<attribute>']`\n",
    "- `<tag>.find('<subtag>').<attribute>` (`<attribute>`: e.g. `text`)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get any 'regular' article from the articles collected above and get attributes of the first <a> tag from it in 5 different ways\n",
    "# (not all attributes will be available using the .<attribute notation>, but the 'text' attribute will)\n",
    "article = articles[0]\n",
    "print(type(article))\n",
    "print(article.find('a', {'class': 'theframe'}))\n",
    "print(article.find('a')['class'])\n",
    "print(article.find('a')['data-image'])\n",
    "print(article.find('a').get('data-image'))\n",
    "print(article.find('a').attrs['data-image'])\n",
    "print(article.find('a').attrs)\n",
    "print(article.find('a').text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Demonstrate `<tag>.find_next_siblings()` (returns all `<tag>`'s siblings) and `<tag>.find_next_sibling()` (returns just the first one)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find the 'div' tag containing the 'class' attribute with the value 'rowline clearfix' (there is only one such a 'div' tag in the soup object so far)\n",
    "rowline_clearfix = soup.find('div', {'class': 'rowline clearfix'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use find_next_sibling() and find_next_siblings() to find 'span' tags in the 'div' tag found in the cell above\n",
    "print(rowline_clearfix.find('span'), '\\n')\n",
    "print(rowline_clearfix.find('span').find_next_sibling(), '\\n')\n",
    "siblings = rowline_clearfix.find('span').find_next_siblings()\n",
    "for sibling in siblings:\n",
    "    print(sibling, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each `bs4.element.ResultSet`, `bs4.element.Tag`,... can be used to create another BeautifulSoup object, using `BeautifulSoup(str(<bs4.element object>), features='html.parser')`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles = BeautifulSoup(str(rowline_clearfix.find_all('article')), 'html.parser')\n",
    "print(type(articles))\n",
    "print(articles.article)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get/Return all text from a `bs4.element.Tag` object, using `<bs4.element.Tag object>.text`, e.g. for an `<article>` tag."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles = articles.find_all('article')\n",
    "print(articles[0].text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get/Return and remove a specific item from a `bs4.element.ResultSet` using `<result set>.pop(<index>)` (default: last)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles.pop(0)\n",
    "print(articles[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting a specific page from a Website where long lists of items are split in multiple pages.\n",
    "The URL of a specific page of such a multi-page Website includes the main part and the suffix. Typical suffixes in the URLs are `'&page=<n>'`, `'&searchpage=<n>'`..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_specific_page(start_url: str, page=1):\n",
    "    \"\"\"Returns a specific page from a Website where long lists of items are split in multiple pages.\n",
    "    \"\"\"\n",
    "\n",
    "    if page > 1:\n",
    "        return start_url.split('&searchpage=')[0] + '&searchpage=' + str(page)\n",
    "    return start_url.split('&searchpage=')[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_specific_page(start_url, page)`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(get_specific_page(start_url, ))\n",
    "print(get_specific_page(start_url, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting the BeautifulSoup object corresponding to a specific page page from a Website where long lists of items are split in multiple pages."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_next_soup(start_url: str, page=1):\n",
    "    \"\"\"Returns the BeautifulSoup object corresponding to a specific page\n",
    "    in case there are multiple pages that list objects of interest.\n",
    "    Parameters:\n",
    "    - start_url: the starting page/url of a multi-page list of objects\n",
    "    - page: the page number of a specific page of a multi-page list of objects\n",
    "    Essentially, get_next_soup() just returns get_soup(get_specific_page(start_url, page)),\n",
    "    i.e. converts the result of the call to get_specific_page(start_url, page), which is a string,\n",
    "    into a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "\n",
    "    return get_soup(get_specific_page(start_url, page))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_next_soup(start_url: str, page=1)`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(get_next_soup(start_url).find('article').text)\n",
    "print(get_next_soup(start_url, 3).find('article').text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Getting the BeautifulSoup object corresponding to a specific page page from a Website where long lists of items are split in multiple pages - the `selenium` version."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_next_soup_selenium(start_url: str, page=1):\n",
    "    \"\"\"Returns the BeautifulSoup object corresponding to a specific page\n",
    "    in case there are multiple pages that list objects of interest, using selenium instead of requests.\n",
    "    Parameters:\n",
    "    - start_url: the starting page/url of a multi-page list of objects\n",
    "    - page: the page number of a specific page of a multi-page list of objects\n",
    "    Essentially, get_next_soup() just returns get_soup_selenium(get_specific_page(start_url, page)),\n",
    "    i.e. converts the result of the call to get_specific_page(start_url, page), which is a string,\n",
    "    into a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "\n",
    "    return get_soup_selenium(get_specific_page(start_url, page))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_next_soup_selenium(start_url: str, page=1)`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(get_next_soup_selenium(start_url).find('article').text)\n",
    "print(get_next_soup_selenium(start_url, 3).find('article').text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Web crawler that collects info about specific articles from Ultimate Classic Rock, implemented as a Python generator."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def crawl(url: str, max_pages=1):\n",
    "    \"\"\"Web crawler that collects info about specific articles from Ultimate Classic Rock,\n",
    "    implemented as a Python generator that yields BeautifulSoup objects (get_next_soup() or get_next_soup_selenium())\n",
    "    from multi-page movie lists.\n",
    "    Parameters: the url of the starting page and the max number of pages to crawl in case of multi-page lists.\n",
    "    \"\"\"\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        yield get_next_soup_selenium(url, page + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `crawl(url: str, max_pages=1)`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_soup = crawl(start_url, 3)\n",
    "while True:\n",
    "    try:\n",
    "        s = next(next_soup)\n",
    "        print(s.article.text)\n",
    "    except StopIteration:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A crawler that returns structured information about articles related to Paul McCartney from Ultimate Classic Rock:\n",
    "    - article title\n",
    "    - article author\n",
    "    - article date\n",
    "    - featured image (URL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_article_info_list(start_url: str, max_pages=1):\n",
    "    \"\"\"\n",
    "    Returns structured information about articles related to Paul McCartney from a multi-page article list.\n",
    "    :param start_url: the url of the starting page of a multi-page article list\n",
    "    :param max_pages: the max number of pages to crawl\n",
    "    :return: a list of tuples of info-items about the articles from a multi-page article list\n",
    "    Creates and uses the following data:\n",
    "    - article title\n",
    "    - article author\n",
    "    - article date\n",
    "    - featured image (URL)\n",
    "    \"\"\"\n",
    "\n",
    "    article_info_list = []\n",
    "    next_soup = crawl(start_url, max_pages)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            s = next(next_soup)\n",
    "            for article in s.find_all('article')[:-1]:\n",
    "                title = article.a.text\n",
    "                author = article.find('div', {'class': 'auth-date'}).find('em').text.lstrip(' by  ')\n",
    "                date = article.find('div', {'class': 'auth-date'}).time.text\n",
    "                image = article.a.attrs['data-image']\n",
    "                article_info_list.append((title, author, date, image))\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "    return article_info_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(article, '\\n')\n",
    "print(article.a.text)\n",
    "print(article.find('div', {'class': 'auth-date'}).find('em').text.lstrip(' by  '))\n",
    "print(article.find('div', {'class': 'auth-date'}).time.text)\n",
    "print(article.a.attrs['data-image'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test `get_articles_info(start_url: str, max_pages=1)`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a_info = get_article_info_list(start_url, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for a in a_info:\n",
    "    print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the info returned by the crawler in a `csv` file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import csv\n",
    "#\n",
    "# csv_file = DATA_DIR / 'articles.csv'\n",
    "# header_row = ['Title', 'Author', 'Date', 'Featured image']\n",
    "# with open(csv_file, 'w', newline='', encoding='utf-8') as f:  # newline: avoid blank rows; encoding: enable ш,š...\n",
    "#     out = csv.writer(f)\n",
    "#     out.writerow(header_row)\n",
    "#     out.writerows(article_info_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "articles_csv = DATA_DIR / 'articles.csv'\n",
    "header = ['Title', 'Author', 'Date', 'Featured image']\n",
    "with open(articles_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "    out = csv.writer(f)\n",
    "    out.writerow(header)\n",
    "    out.writerows(a_info)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And show the `csv` file in Pandas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# article_info_table = pd.read_csv(csv_file)\n",
    "# article_info_table\n",
    "import pandas as pd\n",
    "articles = pd.read_csv(articles_csv)\n",
    "articles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}